{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - TD1\n",
    "\n",
    "By **CHARRIER Timothée** and **RAVELET Thomas**\n",
    "\n",
    "## 1. First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Warning deprecation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing the dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Neural network\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What are the shape of the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Display the dataset shape\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have 70000 images of 28x28 pixels, which is 784 pixels in total. There are 60000 images for the training set and 10000 images for the test set. Here are 10 random images of the training set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random pictures with their labels and axis off\n",
    "\n",
    "fig, ax = plt.subplots(1, 10)\n",
    "for i in range(10):\n",
    "    random_index = random.randint(0, len(X_train))\n",
    "    ax[i].imshow(X_train[random_index], cmap='gray')\n",
    "    ax[i].set_title(\"Label: {}\".format(Y_train[random_index]))\n",
    "    ax[i].axis('off')\n",
    "        \n",
    "plt.suptitle(\"10 random pictures from the MNIST dataset\")\n",
    "fig.set_size_inches(30, 3.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Why split the data into training and test sets is important?\n",
    "\n",
    "The method train_test_split allows to separate the data into two parts: a training part and a test part. This allows to test the model on data that it has not seen during training. This allows to check that the model is not over-trained. Indeed, if the model is over-trained, it will be able to predict the training data but not the test data. This means that the model is not generalizable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Are the training and test sets balanced?\n",
    "\n",
    "To check if the training and test sets are balanced, we can plot the number of images for each digit in percentage. We can see that the training and test sets are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Data balancing \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(np.arange(10), np.bincount(Y_train), color='blue')\n",
    "ax[0].set_title(\"Train set\")\n",
    "ax[0].set_xlabel(\"Class\")\n",
    "ax[0].set_ylabel(\"Number of samples\")\n",
    "ax[1].bar(np.arange(10), np.bincount(Y_test), color='red')\n",
    "ax[1].set_title(\"Test set\")\n",
    "ax[1].set_xlabel(\"Class\")\n",
    "ax[1].set_ylabel(\"Number of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Ecarts types\n",
    "\n",
    "print(\"Train set standard deviation: {:.2f}\".format(np.std(X_train)))\n",
    "print(\"Test set standard deviation: {:.2f}\".format(np.std(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training and test sets are balanced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second part: non-supervised learning\n",
    "\n",
    "### 2.1. Perform a PCA on the training set\n",
    "\n",
    "In this question, we will perform a PCA on the training set with the value of n_components in the following list: [10, 50, 100, 200, 500, 784]. We will then plot a random image of the training set and the corresponding image after the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the dataset\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "\n",
    "n_components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100, 200, 500, 784]\n",
    "random_index = np.random.randint(0, X_train.shape[0])\n",
    "List_explained_variance = []\n",
    "count = 0\n",
    "\n",
    "for n in n_components:\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train_flat)\n",
    "\n",
    "    # Explained variance\n",
    "\n",
    "    List_explained_variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "    # Reconstruct the data and reshape it with the PCA model and display a picture if n = 10, 50, 100, 200, 500, 784\n",
    "    if(n == 10 or n == 50 or n == 100 or n == 200 or n == 500 or n == 784):\n",
    "        X_train_reconstructed = pca.inverse_transform(\n",
    "            pca.transform(X_train_flat))\n",
    "\n",
    "        X_train_reconstructed = X_train_reconstructed.reshape(\n",
    "            X_train.shape[0], 28, 28)\n",
    "\n",
    "        #  Display the reconstructed picture in the same figure\n",
    "\n",
    "        plt.subplot(2, 3, count+1)\n",
    "        plt.imshow(X_train_reconstructed[random_index], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        count += 1\n",
    "\n",
    "        if(n == 784):\n",
    "            plt.title(\"Original\")\n",
    "\n",
    "        else:\n",
    "            plt.title(\"Components = \" + str(n))\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Reconstruction of a random picture of the training set with different number of components\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Explain these values according to your understanding of PCA and use these values to fit a relevant value for n_components.\n",
    "\n",
    "We can see that the more components we keep, the more the image is similar to the original image. However, we can see that the image is not very clear when we keep only 10 components. We can also see that the image is very clear when we keep 784 components. Thus, we can say that the best value for n_components is 784. But the higher the value of n_components, the longer the computation time. Thus, we can choose a value of n_components between 200 and 500 to have a good image and a short computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance\n",
    "\n",
    "plt.plot(n_components, List_explained_variance)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.title(\"Explained variance with different number of components\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting the cumulative explained variance ratio, we can see that the first 200 components explain 90% of the variance. Thus, we can choose a value of n_components between 200 and 500 to have a good image and a short computation time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. With sklearn, perform K-MEANS. Play with the parameter K as well as the initialization (KMEANS++, random, or fixed array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kmeans clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Kmeans clustering with 10 clusters and using kmeans++ initialization vs random initialization\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X_train_flat)\n",
    "\n",
    "kmeans_random = KMeans(n_clusters=10, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "y_kmeans_random = kmeans_random.fit_predict(X_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components\n",
    "\n",
    "pca = PCA(n_components=2).fit(X_train_flat)\n",
    "\n",
    "# Plot the clusters\n",
    "\n",
    "f = plt.figure(figsize=(10, 5))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "\n",
    "ax1.scatter(pca.transform(X_train_flat)[:, 0], pca.transform(X_train_flat)[:, 1], c=y_kmeans)\n",
    "ax1.set_title(\"Kmeans++ initialization\")\n",
    "ax2.scatter(pca.transform(X_train_flat)[:, 0], pca.transform(X_train_flat)[:, 1], c=y_kmeans_random)\n",
    "ax2.set_title(\"Random initialization\")\n",
    "f.colorbar(ax1.scatter(pca.transform(X_train_flat)[:, 0], pca.transform(X_train_flat)[:, 1], c=y_kmeans))\n",
    "f.colorbar(ax2.scatter(pca.transform(X_train_flat)[:, 0], pca.transform(X_train_flat)[:, 1], c=y_kmeans_random))\n",
    "\n",
    "plt.suptitle(\"Kmeans clustering with 10 clusters and using kmeans++ initialization vs random initialization\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two pictures above, we cannot see any difference between the different initializations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. For the correct K (K=10), evaluate how good is this partition (with the knowledge of y).\n",
    "\n",
    "Let's evaluate how good is the partition with the knowledge of y. We can easily see that the partition is not good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how good the partition is\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import rand_score\n",
    "\n",
    "print(\"Adjusted Rand score with kmeans++ initialization: {:.2f}%\".format(rand_score(Y_train, y_kmeans)*100))\n",
    "print(\"Adjusted Rand score with random initialization: {:.2f}%\".format(rand_score(Y_train, y_kmeans_random)*100))\n",
    "\n",
    "print(\"Adjusted Rand score with kmeans++ initialization: {:.2f}%\".format(adjusted_rand_score(Y_train, y_kmeans)*100))\n",
    "print(\"Adjusted Rand score with random initialization: {:.2f}%\".format(adjusted_rand_score(Y_train, y_kmeans_random)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result confirms that the partition is not good.\n",
    "\n",
    "### 2.5. Do the same job with the EM-clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same but using EM clustering\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# EM clustering with 10 clusters with fast convergence\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, covariance_type='full', max_iter=100, random_state=0)\n",
    "y_gmm = gmm.fit_predict(X_train_flat)\n",
    "\n",
    "# Plot the clusters\n",
    "\n",
    "plt.scatter(pca.transform(X_train_flat)[:, 0], pca.transform(X_train_flat)[:, 1], c=y_gmm)\n",
    "plt.colorbar()\n",
    "plt.title(\"EM clustering with 10 clusters with fast convergence\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate how good the partition is\n",
    "\n",
    "print(\"Adjusted Rand score: {:.2f}%\".format(adjusted_rand_score(Y_train, y_gmm)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the EM-clustering, we can see that the partition is not good. Indeed, it is even worse than the K-MEANS partition. We got half of K-MEANS accuracy.\n",
    "This is because the EM-clustering is not adapted to this problem. The EM-clustering is adapted to continuous data. In this case, the data is discrete. Thus, the EM-clustering is not adapted to this problem.\n",
    "\n",
    "## 3. Third part: supervised learning\n",
    "\n",
    "### 3.1. What is the major difference between Naïve Bayes Classifier and Support Vector 1 point Machine (or Logistic Regression)?\n",
    "\n",
    "The major difference between Naïve Bayes Classifier and Support Vector Machine (or Logistic Regression) is that the Naïve Bayes Classifier is a generative model and the Support Vector Machine (or Logistic Regression) is a discriminative model. The generative model tries to model the joint distribution of the data. The discriminative model tries to model the conditional distribution of the data.\n",
    "\n",
    "### 3.2. With sklearn, perform a classification using your favorite methods. With the documentation, check how to modify the parameters and comment how it influences the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Naive Bayes classifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the model\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_flat, Y_train)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "y_pred = gnb.predict(X_test_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(Y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier on reduced data with PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# New training and test set with 100 components\n",
    "\n",
    "pca_100 = PCA(n_components=100).fit(X_train_flat)\n",
    "X_train_pca_100 = pca_100.transform(X_train_flat)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(X_train_pca_100, Y_train)\n",
    "\n",
    "# Test the model\n",
    "\n",
    "X_test_pca_100 = pca_100.transform(X_test_flat)\n",
    "y_pred = svm.predict(X_test_pca_100)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(Y_test, y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. With the score method, compute the accuracy of the model on the training the test datasets. Why do we need to analyze the performance of the model at training and testing time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model on the test set\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(grid.score(X_test_pca_100, Y_test)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to analyze the performance of the model at training and testing time because we need to check that the model is not over-trained. Indeed, if the model is over-trained, it will be able to predict the training data but not the test data. This means that the model is not generalizable. Let's take another example with the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier\n",
    "\n",
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 20)]\n",
    "\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    "    # configure the model\n",
    "    model = DecisionTreeClassifier(max_depth=i)\n",
    "    # fit model on the training dataset\n",
    "    model.fit(X_train_flat, Y_train)\n",
    "    # evaluate on the train dataset\n",
    "    train_yhat = model.predict(X_train_flat)\n",
    "    train_acc = accuracy_score(Y_train, train_yhat)\n",
    "    train_scores.append(train_acc)\n",
    "    # evaluate on the test dataset\n",
    "    test_yhat = model.predict(X_test_flat)\n",
    "    test_acc = accuracy_score(Y_test, test_yhat)\n",
    "    test_scores.append(test_acc)\n",
    "   \n",
    "\n",
    "# plot the depth vs accuracy\n",
    "plt.plot(values, train_scores, '-o', label='Train', color='red')\n",
    "plt.plot(values, test_scores, '-o', label='Test', color='blue')\n",
    "plt.xlabel('Tree depth', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with high values of max_depth, the model is over-trained: there is overfitting. The plot clearly shows that increasing the tree depth in the early stages results in a corresponding improvement in both train and test sets. However, after a certain point, the test set accuracy starts to decrease while the train set accuracy continues to increase. This is a sign of overfitting. Thus, we can say that the best value for max_depth is 10. Indeed, we have 10 digits in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Does the PCA influence the performance of the classification (according to the intensity of the reduction)?\n",
    "\n",
    "The PCA does not influence the performance of the classification. Indeed, we can see that the accuracy is the same with or without the PCA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fourth part: deep learning\n",
    "\n",
    "Same as in 3.1.: we want to perform a supervised image classification task.\n",
    "For a model based on neural networks, the development platform will be TENSORFLOW (with KERAS included).\n",
    "\n",
    "**MultiLayer Perceptron**\n",
    "\n",
    "Your first deep neural network is a Multy Layer Perceptron (MLP), i.e. a feedforward network only composed of fully connected layers. For your first attempt, use only one hidden layer.\n",
    "\n",
    "### 4.1. What is the size of the input tensor? What is the size of the output layer?\n",
    "\n",
    "The size of the input tensor is 784. The size of the output layer is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the input tensor\n",
    "\n",
    "input_tensor = keras.layers.Input(shape=(784,))\n",
    "\n",
    "# Size of the output layer\n",
    "\n",
    "output_layer = keras.layers.Dense(10, activation='softmax')(input_tensor)\n",
    "\n",
    "# Size of the input tensor and output layer\n",
    "\n",
    "print(\"Size of the input tensor: {}\".format(input_tensor.shape))\n",
    "print(\"Size of the output layer: {}\".format(output_layer.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. How many epochs do you use? What does it mean? What is the batch_size? What 1 points does it means?\n",
    "\n",
    "An epoch is an iteration over the entire training data. We use 10 epochs. The batch_size is the number of samples per gradient update. We use a batch_size of 128.\n",
    "In this case, the batch_size means that we will update the weights after 128 samples. To resume the parameters are:\n",
    "\n",
    "    - batch size: the number of training examples used in one forward/backward pass of the neural network model.\n",
    "    - number of epochs: the number of times the training algorithm iterates over the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epochs and batch size\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Why do we define a validation set (for example: validation_split=0.2)?\n",
    "\n",
    "We define a validation set to check that the model is not over-trained. Indeed, if the model is over-trained, it will be able to predict the training data but not the test data. This means that the model is not generalizable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Pick the most important parameters you have to set with the compile and the fit method. Briefly explain why they are important parameters, i.e. they influence the training process.\n",
    "\n",
    "The most important parameters we have to set with the compile method are the optimizer and the loss. The optimizer is the algorithm used to minimize the loss. The loss is the function that is minimized by the optimizer. The most important parameters we have to set with the fit method are the epochs and the batch_size. The epochs is the number of iterations over the entire training data. The batch_size is the number of samples per gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the data to 28x28 pixels\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Convert the data to the right type\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the data to the range of [0, 1]\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above setup the data for the MLP. We reshape the data to have a shape of (60000, 784). We also normalize the data to have values between 0 and 1. We can now build the neural network that use only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 - 4s - loss: 0.2735 - accuracy: 0.9223 - val_loss: 0.1307 - val_accuracy: 0.9615 - 4s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "469/469 - 3s - loss: 0.1105 - accuracy: 0.9683 - val_loss: 0.1081 - val_accuracy: 0.9668 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "469/469 - 3s - loss: 0.0711 - accuracy: 0.9789 - val_loss: 0.0778 - val_accuracy: 0.9745 - 3s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "469/469 - 3s - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.0733 - val_accuracy: 0.9769 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "469/469 - 3s - loss: 0.0373 - accuracy: 0.9891 - val_loss: 0.0621 - val_accuracy: 0.9800 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "469/469 - 3s - loss: 0.0280 - accuracy: 0.9919 - val_loss: 0.0583 - val_accuracy: 0.9823 - 3s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "469/469 - 3s - loss: 0.0208 - accuracy: 0.9946 - val_loss: 0.0629 - val_accuracy: 0.9804 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "469/469 - 3s - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.0624 - val_accuracy: 0.9808 - 3s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "469/469 - 3s - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.0651 - val_accuracy: 0.9804 - 3s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "469/469 - 3s - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.0729 - val_accuracy: 0.9787 - 3s/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Let's build the neural network that only use one hidden layer\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the accuracy and the loss of the model. And also some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot images from the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the first 10 images from the test set and their predicted labels and true labels\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    if np.argmax(predictions[i]) == np.argmax(Y_test[i]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.xlabel(\"Predicted: {}\".format(np.argmax(predictions[i])), color=color)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the prediction is not good. Only 3 out of 10 predictions are correct. We need to improve the model. Let's try to add a second hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the neural network that use two hidden layers\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test, Y_test))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot images from the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the first 10 images from the test set and their predicted labels and true labels\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    if np.argmax(predictions[i]) == np.argmax(Y_test[i]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.xlabel(\"Predicted: {}\".format(np.argmax(predictions[i])), color=color)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a multilayer perceptron, the accuracy of the model rises to over 98%. This neural network uses 4 layers : \n",
    "- 1 input layer which takes the 784 pixels of the picture as argument\n",
    "- 2 hidden layers of 512 perceptrons each with 'relu' as output fonction\n",
    "- 1 output layer with 10 outputs, each one represents the probability of being the digit corresponding to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)**\n",
    "\n",
    "### 4.6 : What is the size of the input tensor? Why it is not the same as for your previous MLP model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 13, 13, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 13, 32)        0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 5408)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                54090     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,410\n",
      "Trainable params: 54,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 11s 21ms/step - loss: 0.3712 - accuracy: 0.8973 - val_loss: 0.1674 - val_accuracy: 0.9517\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.1470 - accuracy: 0.9586 - val_loss: 0.1079 - val_accuracy: 0.9702\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.1062 - accuracy: 0.9697 - val_loss: 0.0833 - val_accuracy: 0.9755\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0833 - accuracy: 0.9755 - 1s/epoch - 3ms/step\n",
      "0.9754999876022339\n",
      "0.08333145081996918\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Reshape the data to 28x28 pixels\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the data to the range of [0, 1]\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "Y_test = np_utils.to_categorical(Y_test, 10)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Create the model with only one hidden layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot images from the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot the first 10 images from the test set and their predicted labels and true labels\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap=plt.cm.binary)\n",
    "    if np.argmax(predictions[i]) == np.argmax(Y_test[i]):\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    plt.xlabel(\"Predicted: {}\".format(np.argmax(predictions[i])), color=color)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cf40499047ac89b49562712a64ea6acf8c6f012e163f0f426c12ed68eaabd48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
